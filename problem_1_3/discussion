Problem 1.3 Discusssion

Running the python script for this problem (python problem_1_3.py) in this repository a number of times we get the following results:

Run 1:

Linear Regression
	MSE without CV: 3449.19
	MSE with CV: 3000.38
Ridge Regression
	MSE without CV: 3831.89
	MSE with CV: 3364.53
Bayesian Ridge Regression
	MSE without CV: 3831.89
	MSE with CV: 2998.91
K-Nearest Neighbors
	MSE without CV: 4348.41
	MSE with CV: 3764.74

Run 2:

Linear Regression
	MSE without CV: 2869.01
	MSE with CV: 3000.38
Ridge Regression
	MSE without CV: 3689.43
	MSE with CV: 3364.53
Bayesian Ridge Regression
	MSE without CV: 3689.43
	MSE with CV: 2998.91
K-Nearest Neighbors
	MSE without CV: 3287.32
	MSE with CV: 3764.74

There are a few interesting results to discuss with these runs. The first thing to note is that all of the models without cross validation perform differently in run 1 vs run 2. This is expected since only splitting the data once, it could be more or less predictive of the testing data depending on how the data is divided up. The second thing to note is that the cross-validation models are consistent between runs. This is expected since regardless of how we split the data, by iterating over each split and randomly assigning it test or train, we would converge to the same model each time. This leads to the next point, in general we would expect the cross-validated models to perform better on any subsequent data, even if it has a higher error rate for a particualr test data set. This is because while one particular split may be predictive for the test data set, it does not mean that it would be a good fit for the subsequent data. 

In terms of the invidaul model type performance we see that all of the linear models perform better than the KNN model, and within the linear models the Bayesian model performs the best of the three. This is unsurprising. Plotting the individual features of dataset we can see that many of the features have a linear relationship. So we would exect that the linear models would predict better than the clustering the KNN model performs. It is also unsurprsing that the Bayesian model performs better than both the Ridge and OLS regressors. This is due to the fact that not all of the features have a linear relationship. It is also interesting to note that the OLS model performs better than the Ridge model, but again this seems to be because the data is already normalized and centered around 0. There are not any rigdes present in the data, so the additional penalty makes the model perform slightl worse than the OLS model. 


 